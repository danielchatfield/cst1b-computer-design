\documentclass{supervision}
\usepackage{course}

\Supervision{5}

\begin{document}

\section*{Lecture 17}

This lecture continues the examination of GPUs and introduces multithreading. At the end, students should understand the basic architecture of a GPU streaming multiprocessor and how this can be programmed in CUDA.

\begin{questions}
    \question
    Why do GPUs rely on multithreading as well as SIMD? What bottleneck does multithreading target?
    \begin{solution}
    It takes a very long time (perhaps hundreds of clock cycles) for a memory operation to complete. CPU architectures include two or three-level cache memory hierarchies to reduce memory latency. Whilst Nvidia's \textit{Kepler} and more recently \textit{Maxwell} architectures do include some caching (e.g. \textit{256KB} on the \textit{GK107} and \textit{2MB} on the \textit{GM107}) they are rarely effective as GPUs are designed for stream computing.
    
    Instead, GPUs tolerate this latency with a high degree of \textit{multithreading}. When one warp stalls on a memory operation the multiprocessor control unit selects another ready warp and switches to that one.
    \end{solution}

    \question
    How can a SIMD processor pipeline be modified to allow multithreading?
    \begin{solution}
    Not sure what this really means. Loading multiple pieces of data at once?
    \end{solution}

    \question
    The warp scheduler chooses instructions to execute. What are its criteria for choosing and what other schemes could be implemented?
    \begin{solution}
    The warp scheduler selects an eligible warp and then depending on which architecture it then dispatches 1 instruction (for Fermi and older) or up to 2 independent instructions (for Kepler and Maxwell).
    \end{solution}

    \question
    What are the types of memory available within a GPU and what are their relative advantages or disadvantages?
    \begin{solution}
    Each SIMD land is allocated private memory for stack frame and spilling registers. Each multithreaded processor has a local memory that is not shared between processors and is allocated dynamically to thread blocks on creation. The processor local memory can be used for communication between threads.
    
    There is also the global GPU memory that is available across the GPU and to the host system.
    \end{solution}

    \question
    What types of application best suit GPUs and how can you program to take advantage of the various forms of parallelism available?
    \begin{solution}
    Any computation that requires the same calculation to be done to lots of data is \textit{embarrassingly parallel} and thus well suited to a GPU. To make best use of the GPU, computations should be programmed such that as many of the SIMD lanes are active.
    \end{solution}

\end{questions}

\section*{Lecture 18}

The final lecture gives an overview of the directions that computer architecture is heading. The aim of this lecture is to give students a perspective on the challenges of the future and an introduction to some of the research that is tackling it.

\begin{questions}
    \question
    Why is energy efficiency the “new fundamental limiter of processor performance”, as Borkar and Chien say?
    \begin{solution}
    In the past, when the transistor size shrunk and more transistors could be put on a die the voltage could also be lowered (keeping the overall power consumption broadly the same). Going forward this will not be possible as the leakage will be too high and thus the energy requirements will be prohibitive. 
    
    The ARM CTO described this as an era of "dark silicon" where a proportion of a chip (potentially a large proportion) at any given time must be idle to meet power (and heat) requirements.
    
    Transistor size is no longer the determining factor in CPU speed and innovation in micro-architecture will be the driving force behind performance gains by making parts more energy efficient.
    \end{solution}

    \question
    Describe ARM’s big.LITTLE system.
    \begin{solution}
    ARM's \textit{big.LITTLE} system is a power saving technology aimed at the mobile market where there is a strong demand for both performance and energy efficiency. It combines a high-performance core with an energy-efficient core, by keeping track of load history during execution it can anticipate the performance needs of the thread next time it runs and runs it on the relevant core.
    \end{solution}

    \begin{parts}
        \part
        What’s the point of having two types of core and when might they be used?
        \begin{solution}
        One core provides high performance (would be used when rendering a webpage) and the other good energy efficiency (would be used when doing some not very demanding task like writing a text message).
        \end{solution}

        \part
        Would it be useful to have a third core type and, if so, what would its characteristics be?
        \begin{solution}
        You could argue that an ultra energy efficient core that was only used for really low performance (but long running) things like tracking motion would be beneficial but you could also argue that it would be better to have that as a co-processor (like the M7 in the iPhone 5S).
        \end{solution}

    \end{parts}
    \question
    When might it make sense to implement functionality in a specialised accelerator rather than within a general purpose core?
    \begin{solution}
    If it is used often and the speedup (or energy efficiency) is significant. An example is video encoding - when implemented as a specialised accelerator it is very efficient as the data can just stream through, without this it would not be possible to achieve 15+ hours video playback on an iPad.
    \end{solution}

    \question
    What types of application domain might benefit from approximate computing?
    \begin{solution}
    Audio and video computation does not need to be exact as humans tend to be quite forgiving of small distortions. If the colour of one in every million pixels was slightly off then it would not even be noticeable.
    \end{solution}


\end{questions}

\section*{Case study}

For a processor or \textit{system-on-chip} of your choice (perhaps one you own), try to find the following:

\begin{questions}
    \begin{solution}
    The Apple A7 is a 64-bit SoC designed by Apple and used in the iPhone 5S. It was the first 64-bit ARM CPU to ship in a consumer smartphone.
    \end{solution}

    \question
    Its main components
    \begin{solution}
    \begin{description}
        \item[Dual core CPU]
        An Apple designed \SIrange{1.3}{1.4}{\GHz} \emph{ARMv8-A} processor called Cyclone.
        \item[GPU]
        A \emph{PowerVR G6430} from Imagination in a four cluster configuration supporting OpenGL ES 3.0.
        \item[Image processor]
        A dedicated image processing chip for reducing noise, sharpening and image stabilizing.
        \item[Secure enclave] 
        Cryptographic chip that stores the data from the Touch ID fingerprint sensor. The security is protected by ARM's TrustZone/SecurCore technology and whilst the firmware can be updated, doing so automatically wipes the flash.
        \item[Motion coprocessor]
        The motion coprocessor is a separate (not actually on the chip so strictly speaking not part of SoC) low power ARM based processor that services the accelerometer, gyroscope and compass. It allows the phone to be continually recording this data and make it available to apps and Apple health.
        
    \end{description}
    \end{solution}

    \question
    Which instruction set(s) and extensions it supports
    \begin{solution}
    It supports the AArch64 and AArch32 instruction sets with TrustZone.
    \end{solution}

    \question
    The core's pipeline structure, including numbers and types of functional units and number of available registers
    \begin{solution}
    The A7 features a 13-stage pipeline and is capable of dispatching up to 6 instructions per cycle. Co-issuing FP and NEON instructions are limited. The pipeline for each core is 3-wide and supports out of order execution. Each core has 4 integer ALUs, 2 load/store units, 2 branch units, 1 indirect branch unit and 3 FP/NEON ALUs.
    
    Apple's LLVM commits indicate a massive 192 entry reorder buffer that lets the CPU reorder instructions to make them more efficient.
    \end{solution}

    \question
    Details of the memory hierarchy
    \begin{solution}
    The SoC contains 1GB of DRAM with an L1 cache consisting of 64KB for instructions and 64KB for data. There is a 1MB L2 cache and a 4MB L3 cache.
    
    The latency on the L1 and L2 caches are 3 and 10 clock cycles respectively (the latter being a huge improvement on the previous generation).
    \end{solution}

    \question
    What it does to reduce the effect of control and data dependencies
    \begin{solution}
    The penalty for mispredicting a branch is between 14 and 19 cycles (it varies). As already mentioned the CPU pipeline reorders instructions (from a huge buffer). The branch predictor is much better (even caused a lawsuit from University of Michigan).
    \end{solution}

    \question
    Its support for parallel execution and memory consistency (if any)
    \begin{solution}
    Is previously mentioned the CPU supports issuing up to 6 instructions simultaneously per core and supports a limited set of NEON instructions (SIMD instructions).
    \end{solution}

    \question
    What it can do to improve performance and/or power efficiency
    \begin{solution} 
    The only real power efficiency improvements over the previous generation are a result of race-to-sleep (by being faster it spends less time working and more time idle) and from the use of the M7 motion co-processor. 
    \end{solution}

    \question
    Its clock speed and fabrication process (e.g. 28/40/65 nm)
    \begin{solution}
    Its clock speed is \SIrange{1.3}{1.4}{\GHz} and is fabricated at \SI{28}{nm}.
    \end{solution}

    \question
    How it has been tailored to its target market, and the compromises which had to be made
    \begin{solution}
    All the components are well suited to being in a mobile phone (e.g. image processor, GPU etc.)
    \end{solution}

    \question
    How it improved on its predecessor (if any) and how it was improved upon by its successor (if any)
    \begin{solution}
    Massive performance improvement (2x) and 64 bit architecture support.
    \end{solution}

\end{questions}



\end{document}
