\documentclass{supervision}
\usepackage{course}

\Supervision{5}

\begin{document}

\section*{Lecture 17}

This lecture continues the examination of GPUs and introduces multithreading. At the end, students should understand the basic architecture of a GPU streaming multiprocessor and how this can be programmed in CUDA.

\begin{questions}
    \question
    Why do GPUs rely on multithreading as well as SIMD? What bottleneck does multithreading target?
    \begin{solution}
    It takes a very long time (perhaps hundreds of clock cycles) for a memory operation to complete. CPU architectures include two or three-level cache memory hierarchies to reduce memory latency. Whilst Nvidia's \textit{Kepler} and more recently \textit{Maxwell} architectures do include some caching (e.g. \textit{256KB} on the \textit{GK107} and \textit{2MB} on the \textit{GM107}) they are rarely effective as GPUs are designed for stream computing.
    
    Instead, GPUs tolerate this latency with a high degree of \textit{multithreading}. When one warp stalls on a memory operation the multiprocessor control unit selects another ready warp and switches to that one.
    \end{solution}

    \question
    How can a SIMD processor pipeline be modified to allow multithreading?
    \begin{solution}
    % TODO SIMD multithreading
    \end{solution}

    \question
    The warp scheduler chooses instructions to execute. What are its criteria for choosing and what other schemes could be implemented?
    \begin{solution}
    The warp scheduler selects an eligible warp and then depending on which architecture it then dispatches 1 instruction (for Fermi and older) or up to 2 independent instructions (for Kepler and Maxwell).
    \end{solution}

    \question
    What are the types of memory available within a GPU and what are their relative advantages or disadvantages?
    \begin{solution}
    Each SIMD land is allocated private memory for stack frame and spilling registers. Each multithreaded processor has a local memory that is not shared between processors and is allocated dynamically to thread blocks on creation. The processor local memory can be used for communication between threads.
    
    There is also the global GPU memory that is available across the GPU and to the host system.
    \end{solution}

    \question
    What types of application best suit GPUs and how can you program to take advantage of the various forms of parallelism available?
    \begin{solution}
    Any computation that requires the same calculation to be done to lots of data is \textit{embarrassingly parallel} and thus well suited to a GPU. To make best use of the GPU, computations should be programmed such that as many of the SIMD lanes are active.
    \end{solution}

\end{questions}

\section*{Lecture 18}

The final lecture gives an overview of the directions that computer architecture is heading. The aim of this lecture is to give students a perspective on the challenges of the future and an introduction to some of the research that is tackling it.

\begin{questions}
    \question
    Why is energy efficiency the “new fundamental limiter of processor performance”, as Borkar and Chien say?
    \begin{solution}
    In the past, when the transistor size shrunk and more transistors could be put on a die the voltage could also be lowered (keeping the overall power consumption broadly the same). Going forward this will not be possible as the leakage will be too high and thus the energy requirements will be prohibitive. 
    
    The ARM CTO described this as an era of "dark silicon" where a proportion of a chip (potentially a large proportion) at any given time must be idle to meet power (and heat) requirements.
    
    Transistor size is no longer the determining factor in CPU speed and innovation in micro-architecture will be the driving force behind performance gains by making parts more energy efficient.
    \end{solution}

    \question
    Describe ARM’s big.LITTLE system.
    \begin{solution}
    ARM's \textit{big.LITTLE} system is a power saving technology aimed at the mobile market where there is a strong demand for both performance and energy efficiency. It combines a high-performance core with an energy-efficient core, by keeping track of load history during execution it can anticipate the performance needs of the thread next time it runs and runs it on the relevant core.
    \end{solution}

    \begin{parts}
        \part
        What’s the point of having two types of core and when might they be used?
        \begin{solution}
        One core provides high performance (would be used when rendering a webpage) and the other good energy efficiency (would be used when doing some not very demanding task like writing a text message).
        \end{solution}

        \part
        Would it be useful to have a third core type and, if so, what would its characteristics be?
        \begin{solution}
        You could argue that an ultra energy efficient core that was only used for really low performance (but long running) things like tracking motion would be beneficial but you could also argue that it would be better to have that as a co-processor (like the M7 in the iPhone 5S).
        \end{solution}

    \end{parts}
    \question
    When might it make sense to implement functionality in a specialised accelerator rather than within a general purpose core?
    \begin{solution}
    If it is used often and the speedup (or energy efficiency) is significant. An example is video encoding - when implemented as a specialised accelerator it is very efficient as the data can just stream through, without this it would not be possible to achieve 15+ hours video playback on an iPad.
    \end{solution}

    \question
    What types of application domain might benefit from approximate computing?
    \begin{solution}
    Audio and video computation does not need to be exact as humans tend to be quite forgiving of small distortions. If the colour of one in every million pixels was slightly off then it would not even be noticeable.
    \end{solution}


\end{questions}

\section*{Case study}

For a processor or \textit{system-on-chip} of your choice (perhaps one you own), try to find the following:

\begin{questions}
    \question
    Its main components
    \question
    Which instruction set(s) and extensions it supports
    \question
    The core's pipeline structure, including numbers and types of functional units and number of available registers
    \question
    Details of the memory hierarchy
    \question
    What it does to reduce the effect of control and data dependencies
    \question
    Its support for parallel execution and memory consistency (if any)
    \question
    What it can do to improve performance and/or power efficiency
    \question
    Its clock speed and fabrication process (e.g. 28/40/65 nm)
    \question
    How it has been tailored to its target market, and the compromises which had to be made
    \question
    How it improved on its predecessor (if any) and how it was improved upon by its successor (if any)
    \question
    Any other noteworthy features or design decisions
    \question
    (Bonus points if you can find or produce a labelled image of the various parts of the chip)
\end{questions}



\end{document}
